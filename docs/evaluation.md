# Evaluation Metrics

This document describes the evaluation metrics used to assess the performance of the RAG system.

- **Retrieval Precision**
- **Answer Relevance**
- **Factual Correctness**
- **Hallucination Detection**
- **Citation Accuracy**
- **Latency**

_See `src/evaluation/metrics.py` for implementation details. More metrics and explanations will be added soon._
